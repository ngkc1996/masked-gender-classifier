{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting patool\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 3.6MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: patool\n",
      "Successfully installed patool-1.12\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install patool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: sudo: not found\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install unrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyunpack\n",
      "  Downloading https://files.pythonhosted.org/packages/83/29/020436b1d8e96e5f26fa282b9c3c13a3b456a36b9ea2edc87c5fed008369/pyunpack-0.2.2-py2.py3-none-any.whl\n",
      "Collecting entrypoint2 (from pyunpack)\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/ca/00c8767568924e5c2209da99b6abdeeed9d11cbae2a713d54d041b092a09/entrypoint2-0.2.3-py2.py3-none-any.whl\n",
      "Collecting easyprocess (from pyunpack)\n",
      "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
      "Collecting argparse (from entrypoint2->pyunpack)\n",
      "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
      "Installing collected packages: argparse, entrypoint2, easyprocess, pyunpack\n",
      "Successfully installed argparse-1.4.0 easyprocess-0.3 entrypoint2-0.2.3 pyunpack-0.2.2\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyunpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (8.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow keras numpy pandas sklearn pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "zf = ZipFile('storage/test.zip', 'r')\n",
    "zf.extractall('storage')\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing unnecessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male', 'female']\n",
      "[]\n",
      "['.ipynb_checkpoints']\n",
      "storage/small_train/female\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "files = folders = 0\n",
    "\n",
    "path = \"storage/train/\"\n",
    "\n",
    "for parent, dirnames, filenames in os.walk(path):\n",
    "    print(dirnames)\n",
    "    for dr in dirnames:\n",
    "        if dr.lower().endswith('.ipynb_checkpoints'):\n",
    "            print(parent)\n",
    "            shutil.rmtree(dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male', 'female']\n",
      "['.ipynb_checkpoints']\n",
      "8,501 files, 3 folders\n"
     ]
    }
   ],
   "source": [
    "#check if everything is removed\n",
    "for _, dirnames, filenames in os.walk(path):\n",
    "  # ^ this idiom means \"we won't be using this value\"\n",
    "    files += len(filenames)\n",
    "    folders += len(dirnames)\n",
    "    if dirnames:\n",
    "        print(dirnames)\n",
    "\n",
    "print(\"{:,} files, {:,} folders\".format(files, folders))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st round of training: Freeze all layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(InceptionV3().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = InceptionV3( input_shape=(299,299,3),\n",
    "                                include_top = False,\n",
    "                                weights = 'imagenet'\n",
    ")\n",
    "\n",
    "#Make all layers non-trainable\n",
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "# Flatten the output layer to 1-dimension\n",
    "x = layers.Flatten()(pre_trained_model.output)\n",
    "# Add fully connected layer, with relu activation\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "# Add droupuut\n",
    "x = layers.Dropout(0.5)(x)\n",
    "# Add sigmoid layer for classification. Sigmoid is used instead of softmax, because this is a binary classifier\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(pre_trained_model.input, x)\n",
    "\n",
    "# binary crossentropy used for binary classification\n",
    "model.compile(optimizer=RMSprop(learning_rate),\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8502 images belonging to 2 classes.\n",
      "Found 1634 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_directory = \"storage/train/\"\n",
    "validation_directory = \"storage/validate/\"\n",
    "seed = 10\n",
    "no_epochs = 40\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen =ImageDataGenerator(rescale=1./255,\n",
    "                                 width_shift_range = 0.1,\n",
    "                                 height_shift_range = 0.1)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_directory, \n",
    "    batch_size = batch_size,\n",
    "    class_mode ='binary',\n",
    "    target_size = (299,299),\n",
    "shuffle = True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    directory=validation_directory, \n",
    "    class_mode ='binary',\n",
    "    target_size = (299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-32b44ef57351>:18: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/40\n",
      "34/34 [==============================] - 482s 14s/step - loss: 3.6530 - accuracy: 0.5574 - val_loss: 0.5467 - val_accuracy: 0.7430\n",
      "Epoch 2/40\n",
      "34/34 [==============================] - 460s 14s/step - loss: 0.7060 - accuracy: 0.6139 - val_loss: 0.6214 - val_accuracy: 0.6383\n",
      "Epoch 3/40\n",
      "34/34 [==============================] - 483s 14s/step - loss: 0.6456 - accuracy: 0.6593 - val_loss: 0.6194 - val_accuracy: 0.6481\n",
      "Epoch 4/40\n",
      "34/34 [==============================] - 489s 14s/step - loss: 0.5822 - accuracy: 0.7031 - val_loss: 0.4777 - val_accuracy: 0.7638\n",
      "Epoch 5/40\n",
      "34/34 [==============================] - 487s 14s/step - loss: 0.5540 - accuracy: 0.7155 - val_loss: 0.4599 - val_accuracy: 0.7785\n",
      "Epoch 6/40\n",
      "34/34 [==============================] - 485s 14s/step - loss: 0.5311 - accuracy: 0.7350 - val_loss: 0.7344 - val_accuracy: 0.6040\n",
      "Epoch 7/40\n",
      "34/34 [==============================] - 487s 14s/step - loss: 0.5106 - accuracy: 0.7542 - val_loss: 0.6043 - val_accuracy: 0.6610\n",
      "Epoch 8/40\n",
      "34/34 [==============================] - 490s 14s/step - loss: 0.4919 - accuracy: 0.7588 - val_loss: 0.4459 - val_accuracy: 0.7864\n",
      "Epoch 9/40\n",
      "34/34 [==============================] - 485s 14s/step - loss: 0.4730 - accuracy: 0.7719 - val_loss: 0.4250 - val_accuracy: 0.7993\n",
      "Epoch 10/40\n",
      "34/34 [==============================] - 487s 14s/step - loss: 0.4642 - accuracy: 0.7792 - val_loss: 0.4001 - val_accuracy: 0.8237\n",
      "Epoch 11/40\n",
      "34/34 [==============================] - 482s 14s/step - loss: 0.4626 - accuracy: 0.7793 - val_loss: 0.4178 - val_accuracy: 0.7987\n",
      "Epoch 12/40\n",
      "34/34 [==============================] - 482s 14s/step - loss: 0.4425 - accuracy: 0.7902 - val_loss: 0.6026 - val_accuracy: 0.6842\n",
      "Epoch 13/40\n",
      "34/34 [==============================] - 485s 14s/step - loss: 0.4397 - accuracy: 0.7897 - val_loss: 0.4952 - val_accuracy: 0.7552\n",
      "Epoch 14/40\n",
      "34/34 [==============================] - 485s 14s/step - loss: 0.4222 - accuracy: 0.8038 - val_loss: 0.3928 - val_accuracy: 0.8213\n",
      "Epoch 15/40\n",
      "34/34 [==============================] - 484s 14s/step - loss: 0.4178 - accuracy: 0.8040 - val_loss: 0.4090 - val_accuracy: 0.8140\n",
      "Epoch 16/40\n",
      "34/34 [==============================] - 486s 14s/step - loss: 0.4208 - accuracy: 0.8042 - val_loss: 0.4022 - val_accuracy: 0.8158\n",
      "Epoch 17/40\n",
      "34/34 [==============================] - 491s 14s/step - loss: 0.3961 - accuracy: 0.8113 - val_loss: 0.3823 - val_accuracy: 0.8262\n",
      "Epoch 18/40\n",
      "34/34 [==============================] - 481s 14s/step - loss: 0.3981 - accuracy: 0.8127 - val_loss: 0.4019 - val_accuracy: 0.8121\n",
      "Epoch 19/40\n",
      "34/34 [==============================] - 488s 14s/step - loss: 0.3825 - accuracy: 0.8233 - val_loss: 0.3820 - val_accuracy: 0.8293\n",
      "Epoch 20/40\n",
      "34/34 [==============================] - 488s 14s/step - loss: 0.3866 - accuracy: 0.8262 - val_loss: 0.3769 - val_accuracy: 0.8335\n",
      "Epoch 21/40\n",
      "34/34 [==============================] - 485s 14s/step - loss: 0.3737 - accuracy: 0.8284 - val_loss: 0.4118 - val_accuracy: 0.8091\n",
      "Epoch 22/40\n",
      "34/34 [==============================] - 485s 14s/step - loss: 0.3870 - accuracy: 0.8233 - val_loss: 0.3742 - val_accuracy: 0.8323\n",
      "Epoch 23/40\n",
      "34/34 [==============================] - 482s 14s/step - loss: 0.3488 - accuracy: 0.8454 - val_loss: 0.4896 - val_accuracy: 0.7778\n",
      "Epoch 24/40\n",
      "34/34 [==============================] - 483s 14s/step - loss: 0.3730 - accuracy: 0.8342 - val_loss: 0.3831 - val_accuracy: 0.8219\n",
      "Epoch 25/40\n",
      "34/34 [==============================] - 482s 14s/step - loss: 0.3508 - accuracy: 0.8417 - val_loss: 0.4696 - val_accuracy: 0.7870\n",
      "Epoch 26/40\n",
      "34/34 [==============================] - 489s 14s/step - loss: 0.3565 - accuracy: 0.8333 - val_loss: 0.3655 - val_accuracy: 0.8427\n",
      "Epoch 27/40\n",
      "34/34 [==============================] - 486s 14s/step - loss: 0.3473 - accuracy: 0.8420 - val_loss: 0.3557 - val_accuracy: 0.8537\n",
      "Epoch 28/40\n",
      "34/34 [==============================] - 481s 14s/step - loss: 0.3418 - accuracy: 0.8467 - val_loss: 0.3730 - val_accuracy: 0.8470\n",
      "Epoch 29/40\n",
      "34/34 [==============================] - 482s 14s/step - loss: 0.3400 - accuracy: 0.8451 - val_loss: 0.3982 - val_accuracy: 0.8341\n",
      "Epoch 30/40\n",
      "34/34 [==============================] - 483s 14s/step - loss: 0.3441 - accuracy: 0.8473 - val_loss: 0.3878 - val_accuracy: 0.8323\n",
      "Epoch 31/40\n",
      "34/34 [==============================] - 475s 14s/step - loss: 0.3364 - accuracy: 0.8458 - val_loss: 0.3680 - val_accuracy: 0.8317\n",
      "Epoch 32/40\n",
      "34/34 [==============================] - 475s 14s/step - loss: 0.3219 - accuracy: 0.8582 - val_loss: 0.3581 - val_accuracy: 0.8384\n",
      "Epoch 33/40\n",
      "34/34 [==============================] - 454s 13s/step - loss: 0.3264 - accuracy: 0.8585 - val_loss: 0.4701 - val_accuracy: 0.7766\n",
      "Epoch 34/40\n",
      "34/34 [==============================] - 455s 13s/step - loss: 0.3310 - accuracy: 0.8492 - val_loss: 0.3744 - val_accuracy: 0.8280\n",
      "Epoch 35/40\n",
      "34/34 [==============================] - 461s 14s/step - loss: 0.3088 - accuracy: 0.8627 - val_loss: 0.4437 - val_accuracy: 0.8097\n",
      "Epoch 36/40\n",
      "34/34 [==============================] - 457s 13s/step - loss: 0.3083 - accuracy: 0.8625 - val_loss: 0.4268 - val_accuracy: 0.8201\n",
      "Epoch 37/40\n",
      "34/34 [==============================] - 453s 13s/step - loss: 0.3036 - accuracy: 0.8681 - val_loss: 0.3733 - val_accuracy: 0.8317\n",
      "Epoch 38/40\n",
      "34/34 [==============================] - 455s 13s/step - loss: 0.3205 - accuracy: 0.8604 - val_loss: 0.3532 - val_accuracy: 0.8488\n",
      "Epoch 39/40\n",
      "34/34 [==============================] - 458s 13s/step - loss: 0.2772 - accuracy: 0.8783 - val_loss: 0.3660 - val_accuracy: 0.8476\n",
      "Epoch 40/40\n",
      "34/34 [==============================] - 455s 13s/step - loss: 0.2910 - accuracy: 0.8744 - val_loss: 0.3587 - val_accuracy: 0.8537\n"
     ]
    }
   ],
   "source": [
    "checkpoint_filepath = 'storage/checkpoint/inception_freeze_all.hdf5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                    validation_data=validation_generator, \n",
    "                    epochs=no_epochs, \n",
    "                    verbose=1,\n",
    "                   callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss= history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(accuracy)\n",
    "plt.plot(val_accuracy)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('storage/results/inception_freeze_all_accuracy.png')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('storage/results/inception_freeze_all_loss.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "weights_dir = \"storage/checkpoint/best_model.hdf5\"\n",
    "model.load_weights(weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "num_female = sum(len(files) for _, _, files in os.walk('storage/test/female'))\n",
    "num_male = sum(len(files) for _, _, files in os.walk('storage/test/male'))\n",
    "print(num_female + num_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1647 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From <ipython-input-8-2ef17beb942f>:14: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "103/103 [==============================] - 69s 666ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_directory = \"storage/test/\"\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "generator = test_datagen.flow_from_directory(\n",
    "        test_directory,\n",
    "        target_size=(299, 299),\n",
    "        batch_size=16,\n",
    "        class_mode=None,  # only data, no labels\n",
    "        shuffle=False)  # keep data in same order as labels\n",
    "\n",
    "probabilities = model.predict_generator(generator, verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[781 105]\n",
      " [139 622]]\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([0] * num_female + [1] * num_male)\n",
    "y_pred = probabilities > 0.5\n",
    "\n",
    "res = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(res, cmap='Blues')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax.text(j, i, res[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"y\")\n",
    "plt.title('Confusion matrix from model prediction')\n",
    "plt.savefig('storage/results/inception_freeze_all_loss_confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd round of training: Unfreeze last 2 inception blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n"
     ]
    }
   ],
   "source": [
    "pre_trained_model_v2 = InceptionV3( input_shape=(299,299,3),\n",
    "                                include_top = False,\n",
    "                                weights = 'imagenet'\n",
    ")\n",
    "print(len(pre_trained_model_v2.layers))\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in pre_trained_model_v2.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in pre_trained_model_v2.layers[249:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "# Flatten the output layer to 1-dimension\n",
    "x = layers.Flatten()(pre_trained_model_v2.output)\n",
    "# Add fully connected layer, with relu activatio\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "# Add droupuut\n",
    "x = layers.Dropout(0.5)(x)\n",
    "# Add sigmoid layer for classification. Sigmoid is used instead of softmax, because this is a binary classifier\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_v2 = Model(pre_trained_model_v2.input, x)\n",
    "\n",
    "# binary crossentropy used for binary classification\n",
    "model_v2.compile(optimizer=RMSprop(learning_rate),\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up train and validation generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8502 images belonging to 2 classes.\n",
      "Found 1634 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_directory = \"storage/train/\"\n",
    "validation_directory = \"storage/validate/\"\n",
    "seed = 10\n",
    "no_epochs = 30\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen =ImageDataGenerator(rescale=1./255,\n",
    "                                 width_shift_range = 0.1,\n",
    "                                 height_shift_range = 0.1)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_directory, \n",
    "    batch_size = batch_size,\n",
    "    class_mode ='binary',\n",
    "    target_size = (299,299),\n",
    "shuffle = True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    directory=validation_directory, \n",
    "    class_mode ='binary',\n",
    "    target_size = (299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-2c9f00237a68>:16: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/30\n",
      "34/34 [==============================] - 593s 17s/step - loss: 3.7615 - accuracy: 0.5859 - val_loss: 0.9795 - val_accuracy: 0.4865\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 584s 17s/step - loss: 0.4949 - accuracy: 0.7677 - val_loss: 0.3685 - val_accuracy: 0.8476\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 663s 20s/step - loss: 0.3529 - accuracy: 0.8502 - val_loss: 0.8785 - val_accuracy: 0.6377\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 836s 25s/step - loss: 0.2601 - accuracy: 0.8954 - val_loss: 0.4204 - val_accuracy: 0.8213\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 839s 25s/step - loss: 0.1821 - accuracy: 0.9281 - val_loss: 0.2732 - val_accuracy: 0.8972\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 807s 24s/step - loss: 0.1348 - accuracy: 0.9518 - val_loss: 0.2630 - val_accuracy: 0.8947\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 571s 17s/step - loss: 0.1033 - accuracy: 0.9635 - val_loss: 0.3185 - val_accuracy: 0.8996\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 570s 17s/step - loss: 0.0854 - accuracy: 0.9729 - val_loss: 0.3618 - val_accuracy: 0.8947\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 567s 17s/step - loss: 0.0664 - accuracy: 0.9744 - val_loss: 0.4847 - val_accuracy: 0.8825\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 562s 17s/step - loss: 0.0614 - accuracy: 0.9766 - val_loss: 0.5075 - val_accuracy: 0.8727\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 570s 17s/step - loss: 0.0472 - accuracy: 0.9839 - val_loss: 0.4008 - val_accuracy: 0.9033\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - 567s 17s/step - loss: 0.0484 - accuracy: 0.9847 - val_loss: 0.4703 - val_accuracy: 0.8990\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - 563s 17s/step - loss: 0.0395 - accuracy: 0.9868 - val_loss: 0.5397 - val_accuracy: 0.8917\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - 565s 17s/step - loss: 0.0494 - accuracy: 0.9838 - val_loss: 0.5289 - val_accuracy: 0.8941\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - 568s 17s/step - loss: 0.0357 - accuracy: 0.9889 - val_loss: 0.5696 - val_accuracy: 0.9015\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - 592s 17s/step - loss: 0.0334 - accuracy: 0.9879 - val_loss: 0.4532 - val_accuracy: 0.9186\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - 570s 17s/step - loss: 0.0339 - accuracy: 0.9896 - val_loss: 0.8820 - val_accuracy: 0.8580\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - 612s 18s/step - loss: 0.0320 - accuracy: 0.9885 - val_loss: 0.7316 - val_accuracy: 0.8813\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - 571s 17s/step - loss: 0.0235 - accuracy: 0.9928 - val_loss: 0.4708 - val_accuracy: 0.9155\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - 600s 18s/step - loss: 0.0325 - accuracy: 0.9904 - val_loss: 0.5076 - val_accuracy: 0.9162\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - 572s 17s/step - loss: 0.0213 - accuracy: 0.9928 - val_loss: 0.5676 - val_accuracy: 0.9082\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - 568s 17s/step - loss: 0.0304 - accuracy: 0.9898 - val_loss: 0.4945 - val_accuracy: 0.9174\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - 571s 17s/step - loss: 0.0255 - accuracy: 0.9922 - val_loss: 0.7072 - val_accuracy: 0.9033\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - 571s 17s/step - loss: 0.0227 - accuracy: 0.9936 - val_loss: 0.5700 - val_accuracy: 0.9106\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - 570s 17s/step - loss: 0.0211 - accuracy: 0.9933 - val_loss: 0.5914 - val_accuracy: 0.9076\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - 569s 17s/step - loss: 0.0242 - accuracy: 0.9933 - val_loss: 2.2478 - val_accuracy: 0.7907\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - 571s 17s/step - loss: 0.0226 - accuracy: 0.9931 - val_loss: 1.3846 - val_accuracy: 0.8470\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - 573s 17s/step - loss: 0.0150 - accuracy: 0.9953 - val_loss: 0.7206 - val_accuracy: 0.9088\n",
      "Epoch 29/30\n",
      "34/34 [==============================] - 737s 22s/step - loss: 0.0205 - accuracy: 0.9942 - val_loss: 0.9115 - val_accuracy: 0.8856\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - 795s 23s/step - loss: 0.0194 - accuracy: 0.9942 - val_loss: 0.8042 - val_accuracy: 0.8947\n"
     ]
    }
   ],
   "source": [
    "checkpoint_filepath = 'storage/checkpoint/inception_fine_tune.hdf5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model_v2.fit_generator(generator=train_generator,\n",
    "                    validation_data=validation_generator, \n",
    "                    epochs=no_epochs, \n",
    "                    verbose=1,\n",
    "                   callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss= history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(accuracy)\n",
    "plt.plot(val_accuracy)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('storage/results/inception_fine_tune_accuracy.png')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('storage/results/inception_fine_tune_loss.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "weights_dir = \"storage/checkpoint/inception_fine_tune.hdf5\"\n",
    "model_v2.load_weights(weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "num_female = sum(len(files) for _, _, files in os.walk('storage/test/female'))\n",
    "num_male = sum(len(files) for _, _, files in os.walk('storage/test/male'))\n",
    "print(num_female + num_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1647 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From <ipython-input-10-d4a974b87e3a>:14: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "103/103 [==============================] - 70s 682ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_directory = \"storage/test/\"\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "generator = test_datagen.flow_from_directory(\n",
    "        test_directory,\n",
    "        target_size=(299, 299),\n",
    "        batch_size=16,\n",
    "        class_mode=None,  # only data, no labels\n",
    "        shuffle=False)  # keep data in same order as labels\n",
    "\n",
    "probabilities = model_v2.predict_generator(generator, verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[822  64]\n",
      " [ 62 699]]\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([0] * num_female + [1] * num_male)\n",
    "y_pred = probabilities > 0.5\n",
    "\n",
    "res = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(res, cmap='Blues')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax.text(j, i, res[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"y\")\n",
    "plt.title('Confusion matrix from model_v2 prediction')\n",
    "plt.savefig('storage/results/inception_fine_tune.png')\n",
    "plt.close()\n",
    "\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
